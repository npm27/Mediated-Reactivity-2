3600-1697-300-1000
library(lrd)
write.csv(lrd::answer_key_free2, file = "test_key.csv", row.names = F)
lrd::free_data
?arrange_data
arrange_data(lrd::free_data, responses = Response, sep = " ",
Username)
arrange_data(lrd::free_data, responses = Response, sep = " ",
"Username")
arrange_data(lrd::free_data, responses = "Response", sep = " ",
"Username")
lrd::free_data
lrd::free_data
write.csv(arrange_data(lrd::free_data, responses = "Response", sep = " ",
"Username"), file = "test.csv", row.names = F)
answer_key_free2
wide_data
answer_key_free
write.csv(lrd::answer_key_free, file = "test_key.csv", row.names = F)
write.csv(arrange_data(lrd::wide_data, responses = "Response", sep = " ",
"Username"), file = "test.csv", row.names = F)
write.csv(arrange_data(lrd::wide_data, responses = "Response", sep = ",",
"Username"), file = "test.csv", row.names = F)
key = lrd::answer_key_free
all_lags <- sort(c((1:(nrow(key) - 1)) * -1, 1:(nrow(key) - 1)))
merge_lags <- data.frame(Sub.ID = rep(unique(DF$Sub.ID), each = length(all_lags)),
participant_lags = rep(all_lags, length(unique(DF$Sub.ID))))
View(all_lags)
####Set up####
##read in the data
dat = read.csv("Data/JOL_data.csv")
##Load libraries
library(ez)
library(psychReport)
library(reshape)
##turn off scientific notation
options(scipen = 999)
#check ethnicities
table(dat$participant.ethnicity) / 40
(48 * 20) + (15 * 26)
x = c(1,7,9,12,15,2,8,3,2)
mean(x)
sd(x)
setwd("C:/Users/nickm/OneDrive/Documents/GitHub/ED-JOLs/4 Analyses")
setwd("C:/Users/nickm/OneDrive/Documents/GitHub/ED-JOLs/4 Analyses/Ex 1A")
####Set up####
##read in data
dat = read.csv("Ex 1A scored.csv")
##turn off scientific notation
options(scipen = 999)
##load libraries
library(ez)
library(reshape)
library(psychReport)
View(dat)
####format the dataset
##drop the pilot data
dat = subset(dat,
dat$Source != "MSU")
View(dat)
##get groups
read = subset(dat,
dat$encoding == "read")
term = subset(dat,
dat$encoding == "term")
glob = subset(dat,
dat$encoding == "glob")
glob = subset(dat,
dat$encoding == "global")
##get starting ns
length(unique(dat$ID))
length(unique(read$ID))
length(unique(term$ID))
length(unique(glob$ID))
##Convert Ys and Ns to 1s and 0s
dat$Correct.[dat$Correct. == "Y"] = "1"
dat$Correct.[dat$Correct. == "N"] = "0"
dat$Correct. == as.numeric(dat$Correct.)
dat$Correct. = as.numeric(dat$Correct.)
colnames(dat)[8] = "scored"
##get groups
read = subset(dat,
dat$encoding == "read")
term = subset(dat,
dat$encoding == "term")
glob = subset(dat,
dat$encoding == "global")
##get starting ns
length(unique(dat$ID)) #106 (overall)
length(unique(read$ID)) #35 -- read
length(unique(term$ID)) #34 -- term (item)
length(unique(glob$ID)) #37 -- global
##get overall descriptives
tapply(dat$scored, dat$encoding, mean)
View(glob)
####Data Cleaning####
##remove < 5%; > 95% (will also check JOL data to ensure that they stayed on task)
dat2 = cast(dat, ID ~ encoding, mean)
View(dat2)
#one term and global at 0%
dat = subset(dat, dat$ID != "3023" | dat$ID != "2014")
#one term and global at 0%
dat = subset(dat, dat$ID != "3023" & dat$ID != "2014")
#re-run descriptives
tapply(dat$scored, dat$encoding, mean)
View(dat)
####Run the ANOVA####
#one-way, between subjects
model1 = ezANOVA(dat,
dv = scored,
between = encoding,
wid = ID,
type = 3,
detailed = T)
model1
####Set up####
##read in data
dat = read.csv("Ex 1A scored.csv")
##turn off scientific notation
options(scipen = 999)
##load libraries
library(ez)
library(reshape)
library(psychReport)
####format the dataset
##drop the pilot data
##Convert Ys and Ns to 1s and 0s
dat$Correct.[dat$Correct. == "Y"] = "1"
dat$Correct.[dat$Correct. == "N"] = "0"
dat$Correct. = as.numeric(dat$Correct.)
colnames(dat)[8] = "scored"
##get groups
read = subset(dat,
dat$encoding == "read")
term = subset(dat,
dat$encoding == "term")
glob = subset(dat,
dat$encoding == "global")
##get starting ns
length(unique(dat$ID)) #106 (overall)
length(unique(read$ID)) #35 -- read (34)
length(unique(term$ID)) #34 -- term (item)
length(unique(glob$ID)) #37 -- global (36)
##get overall descriptives
tapply(dat$scored, dat$encoding, mean) #looks like no reactivity; possible negative reactivty on item-level JOLs
####Data Cleaning####
##remove < 5%; > 95% (will also check JOL data to ensure that they stayed on task)
dat2 = cast(dat, ID ~ encoding, mean)
#one term and global at 0%
dat = subset(dat, dat$ID != "3023" & dat$ID != "2014")
#re-run descriptives
tapply(dat$scored, dat$encoding, mean)
####Run the ANOVA####
#one-way, between subjects
model1 = ezANOVA(dat,
dv = scored,
between = encoding,
wid = ID,
type = 3,
detailed = T)
model1 #non-significant
length(unique(read$ID)) #35 -- read (34)
length(unique(term$ID)) #34 -- term (item)
length(unique(glob$ID)) #37 -- global (36)
####Set up####
##read in data
dat = read.csv("Ex 1A scored.csv")
##turn off scientific notation
options(scipen = 999)
##load libraries
library(ez)
library(reshape)
library(psychReport)
##Convert Ys and Ns to 1s and 0s
dat$Correct.[dat$Correct. == "Y"] = "1"
dat$Correct.[dat$Correct. == "N"] = "0"
dat$Correct. = as.numeric(dat$Correct.)
colnames(dat)[8] = "scored"
##get groups
read = subset(dat,
dat$encoding == "read")
term = subset(dat,
dat$encoding == "term")
glob = subset(dat,
dat$encoding == "global")
##get starting ns
length(unique(dat$ID)) #106 (overall)
length(unique(read$ID)) #35 -- read (34)
View(dat)
##fix subject IDs
dat.MSU = subset(dat,
dat$Source == "MSU")
dat.P = subset(dat,
dat$Source != "MSU")
View(dat)
dat.MSU$ID = dat.MSU$ID + 10000
dat = rbind(dat.MSU, dat.p)
dat = rbind(dat.MSU, dat.P)
##get groups
read = subset(dat,
dat$encoding == "read")
term = subset(dat,
dat$encoding == "term")
glob = subset(dat,
dat$encoding == "global")
##get starting ns
length(unique(dat$ID)) #106 (overall)
length(unique(read$ID)) #35 -- read (34)
length(unique(term$ID)) #34 -- term (item) | w/msu ()
##get starting ns
length(unique(dat$ID)) #106 (overall)      | w/msu
length(unique(glob$ID)) #37 -- global (36) |
##get overall descriptives
tapply(dat$scored, dat$encoding, mean) #looks like no reactivity; possible negative reactivty on item-level JOLs
####Data Cleaning####
##remove < 5%; > 95% (will also check JOL data to ensure that they stayed on task)
dat2 = cast(dat, ID ~ encoding, mean)
View(dat2)
#one term and global at 0%
dat = subset(dat, dat$ID != "3023" & dat$ID != "2014")
#re-run descriptives
tapply(dat$scored, dat$encoding, mean)
####Run the ANOVA####
#one-way, between subjects
model1 = ezANOVA(dat,
dv = scored,
between = encoding,
wid = ID,
type = 3,
detailed = T)
model1 #non-significant
####Set up####
##read in data
dat = read.csv("Ex 1A scored.csv")
View(dat)
##turn off scientific notation
options(scipen = 999)
##load libraries
library(ez)
library(reshape)
library(psychReport)
##Convert Ys and Ns to 1s and 0s
dat$Correct.[dat$Correct. == "Y"] = "1"
dat$Correct.[dat$Correct. == "N"] = "0"
dat$Correct. = as.numeric(dat$Correct.)
colnames(dat)[8] = "scored"
##fix subject IDs
dat.MSU = subset(dat,
dat$Source == "MSU")
dat.P = subset(dat,
dat$Source != "MSU")
dat.MSU$ID = dat.MSU$ID + 10000
dat = rbind(dat.MSU, dat.P)
##get groups
read = subset(dat,
dat$encoding == "read")
term = subset(dat,
dat$encoding == "term")
glob = subset(dat,
dat$encoding == "global")
##get starting ns
length(unique(dat$ID)) #106 (overall)      | w/msu (113)
length(unique(read$ID)) #35 -- read (34)   | w/msu (37)
length(unique(term$ID)) #34 -- term (item) | w/msu (36)
length(unique(glob$ID)) #37 -- global (36) | w/msu (40)
##get overall descriptives
tapply(dat$scored, dat$encoding, mean) #looks like no reactivity; possible negative reactivty on item-level JOLs
####Data Cleaning####
##remove < 5%; > 95% (will also check JOL data to ensure that they stayed on task)
dat2 = cast(dat, ID ~ encoding, mean)
#one term and global at 0%
dat = subset(dat, dat$ID != "3023" & dat$ID != "2014")
####Run the ANOVA####
#one-way, between subjects
model1 = ezANOVA(dat,
dv = scored,
between = encoding,
wid = ID,
type = 3,
detailed = T)
model1 #non-significant
#re-run descriptives
tapply(dat$scored, dat$encoding, mean)
setwd("C:/Users/nickm/OneDrive/Documents")
n = read.csv(ns.csv)
n = read.csv("ns.csv")
View(n)
length(unique(n$Username))
MX = 54.20
MY = 58.28
MDIFF = 4.08
SDX = 11.89
SDY = 9.95
SDDIFF = 5.41
RXX = .503
RYY = .236
rXY = .892
(SDX^2 * RXX) + (SDY^2 * RYY) - (2 * rXY * SDX * SDY)
((SDX^2 * RXX) + (SDY^2 * RYY) - (2 * rXY * SDX * SDY)) / (SDX^2 + SDY^2) - (2 * rxy *SDX * SDY)
((SDX^2 * RXX) + (SDY^2 * RYY) - (2 * rXY * SDX * SDY)) / (SDX^2 + SDY^2) - (2 * rxy *SDX * SDY)
((SDX^2 * RXX) + (SDY^2 * RYY) - (2 * rXY * SDX * SDY)) / (SDX^2 + SDY^2) - (2 * Rxy *SDX * SDY)
((SDX^2 * RXX) + (SDY^2 * RYY) - (2 * rXY * SDX * SDY)) / (SDX^2 + SDY^2) - (2 * rXY *SDX * SDY)
((SDX^2 * RXX) + (SDY^2 * RYY) - (2 * rXY * SDX * SDY))
(SDX^2 + SDY^2) - (2 * rXY *SDX * SDY)
SDX^2
SDY^2
2 * rXY * SDX * SDY
2 * rXY *SDX * SDY
(SDX^2 + SDY^2)
(SDX^2 + SDY^2 - 2 * rXY *SDX * SDY)
((SDX^2 * RXX) + (SDY^2 * RYY) - (2 * rXY * SDX * SDY))
((SDX^2 * RXX) + (SDY^2 * RYY) - (2 * rXY * SDX * SDY)) / (SDX^2 + SDY^2 - 2 * rXY *SDX * SDY)
setwd("~/GitHub/Mediated-Reactivity-2/2 Output/Ex 2A/MSU")
setwd("~/GitHub/Mediated-Reactivity-2/2 Output/Ex 2A/MSU")
####This script will be used to read in everything and set EX 2data up for processing####
##Start by gathering all of the data
#JOLs and frequency
setwd("C:/Users/nickm/OneDrive/Documents/GitHub/Mediated-Reactivity-2/2 Output/Ex 2A/MSU/JOL")
files = list.files(pattern = "*.csv")
#Put them in one dataframe. First apply read.csv, then rbind
dat = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
#get the number of participants
length(unique(dat$Username))
#read
setwd("C:/Users/nickm/OneDrive/Documents/GitHub/Mediated-Reactivity-2/2 Output/Ex 2A/MSU/Read")
files2 = list.files(pattern = "*.csv")
#Put them in one dataframe. First apply read.csv, then rbind
dat2 = do.call(rbind, lapply(files2, function(x) read.csv(x, stringsAsFactors = FALSE)))
#get the number of participants
length(unique(dat2$Username))
#Now move back to the original folder
#This is where I'll store the combined final output for scoring
setwd('..')
####Clean up the data files####
##Drop unused columns
dat = dat[ , -c(2:4, 6:7, 9:10, 12, 20:23, 27:32, 34)]
dat2 = dat2[ , -c(2:4, 6:7, 9:10, 12, 20:23, 27:33)]
#Next, remove buffer trials
dat = subset(dat,
dat$Stimuli.Stimuli.Notes != "Buffer")
dat2 = subset(dat2,
dat2$Stimuli.Stimuli.Notes != "Buffer")
#Now remove instruction trials
dat = subset(dat,
dat$Procedure.Trial.Type != "Instruct")
dat2 = subset(dat2,
dat2$Procedure.Trial.Type != "Instruct")
#Now remove filler task
dat = subset(dat,
dat$Procedure.Trial.Type != "FreeRecall")
dat2 = subset(dat2,
dat2$Procedure.Trial.Type != "FreeRecall")
####Set the data up for scoring####
#Start by subsetting out the recall and JOL data for each dataset
dat.JOL = subset(dat,
dat$Procedure.Trial.Type == "JOL")
dat.Recall = subset(dat,
dat$Procedure.Trial.Type == "Test")
#get JOLs and Recall in the same order
dat.JOL = dat.JOL[order(dat.JOL$Stimuli.Cue), ]
dat.JOL = dat.JOL[order(dat.JOL$Condition.Number), ]
dat.JOL = dat.JOL[order(dat.JOL$Stimuli.Shuffle), ]
dat.Recall = dat.Recall[order(dat.Recall$Stimuli.Cue), ]
dat.Recall = dat.Recall[order(dat.Recall$Condition.Number), ]
dat.Recall = dat.Recall[order(dat.Recall$Stimuli.Shuffle), ]
#Okay, put it back together now
dat.R = dat.Recall[ , c(12:14)]
#Drop overlapping columns and clean things up
JOL = cbind(dat.JOL, dat.R)
JOL = JOL[ , -c(11, 14, 17)]
JOL = JOL[ , -c(9:10)]
JOL = JOL[ , -2]
##Now do the same for the study only condition
#Start by subsetting out the recall and study trials for each dataset
dat2.Study = subset(dat2,
dat2$Procedure.Trial.Type == "Study")
dat2.Recall = subset(dat2,
dat2$Procedure.Trial.Type == "Test")
#get Study and Recall in the same order
dat2.Study = dat2.Study[order(dat2.Study$Stimuli.Cue), ]
dat2.Study = dat2.Study[order(dat2.Study$Condition.Number), ]
dat2.Study = dat2.Study[order(dat2.Study$Stimuli.Shuffle), ]
dat2.Recall = dat2.Recall[order(dat2.Recall$Stimuli.Cue), ]
dat2.Recall = dat2.Recall[order(dat2.Recall$Condition.Number), ]
dat2.Recall = dat2.Recall[order(dat2.Recall$Stimuli.Shuffle), ]
#Okay, put it back together now
dat2.R = dat2.Recall[ , c(12:14)]
#Drop overlapping columns and clean things up
Study = cbind(dat2.Study, dat2.R)
Study = Study[ , -c(11, 14, 16)]
Study = Study[ , -c(9:10)]
Study = Study[ , -2]
####Score the recall data####
##Going to write everything to .csv and then use the old shiny app to score
#first lowercase everything
JOL$Response.Response[JOL$Response.Response == 	"beyonc�"] = ""
JOL$Stimuli.Cue = tolower(JOL$Stimuli.Cue)
JOL$Stimuli.Answer = tolower(JOL$Stimuli.Answer)
Study$Stimuli.Cue = tolower(Study$Stimuli.Cue)
Study$Stimuli.Answer = tolower(Study$Stimuli.Answer)
JOL$Response.Response = tolower(JOL$Response.Response)
Study$Response.Response = tolower(Study$Response.Response)
#now write to .csv for scoring
length(unique(JOL$Username)) #12
length(unique(Study$Username)) #8
#add source
JOL$source = rep("MSU")
Study$source = rep("MSU")
#(JOL[ , c(1, 12, 5, 2:4, 6:11, 13)], file = "JOL_pre_scored_msu.csv", row.names = F)
#write.csv(Study[ , c(1, 11, 5, 2:4, 6:10, 12)], file = "Study_pre_scored_msu.csv", row.names = F)
write.csv(JOL[ , c(1, 12, 5, 2:4, 6:11, 13)], file = "JOL_pre_scored_msu.csv", row.names = F)
write.csv(Study[ , c(1, 11, 5, 2:4, 6:10, 12)], file = "Study_pre_scored_msu.csv", row.names = F)
setwd("~/GitHub/Mediated-Reactivity-2/2 Output/Ex 2A/USM")
####This script will be used to read in everything and set EX 2data up for processing####
##Start by gathering all of the data
#JOLs and frequency
setwd("C:/Users/nickm/OneDrive/Documents/GitHub/Mediated-Reactivity-2/2 Output/Ex 2A/USM/JOL")
files = list.files(pattern = "*.csv")
#Put them in one dataframe. First apply read.csv, then rbind
dat = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
#get the number of participants
length(unique(dat$Username))
#read
setwd("C:/Users/nickm/OneDrive/Documents/GitHub/Mediated-Reactivity-2/2 Output/Ex 2A/USM/Read")
files2 = list.files(pattern = "*.csv")
#Put them in one dataframe. First apply read.csv, then rbind
dat2 = do.call(rbind, lapply(files2, function(x) read.csv(x, stringsAsFactors = FALSE)))
#get the number of participants
length(unique(dat2$Username))
#Now move back to the original folder
#This is where I'll store the combined final output for scoring
setwd('..')
####Clean up the data files####
##Drop unused columns
dat = dat[ , -c(2:4, 6:7, 9:10, 12, 20:23, 27:32, 34)]
dat2 = dat2[ , -c(2:4, 6:7, 9:10, 12, 20:23, 27:33)]
#Next, remove buffer trials
dat = subset(dat,
dat$Stimuli.Stimuli.Notes != "Buffer")
dat2 = subset(dat2,
dat2$Stimuli.Stimuli.Notes != "Buffer")
#Now remove instruction trials
dat = subset(dat,
dat$Procedure.Trial.Type != "Instruct")
dat2 = subset(dat2,
dat2$Procedure.Trial.Type != "Instruct")
#Now remove filler task
dat = subset(dat,
dat$Procedure.Trial.Type != "FreeRecall")
dat2 = subset(dat2,
dat2$Procedure.Trial.Type != "FreeRecall")
####Set the data up for scoring####
#Start by subsetting out the recall and JOL data for each dataset
dat.JOL = subset(dat,
dat$Procedure.Trial.Type == "JOL")
dat.Recall = subset(dat,
dat$Procedure.Trial.Type == "Test")
#get JOLs and Recall in the same order
dat.JOL = dat.JOL[order(dat.JOL$Stimuli.Cue), ]
dat.JOL = dat.JOL[order(dat.JOL$Condition.Number), ]
dat.JOL = dat.JOL[order(dat.JOL$Stimuli.Shuffle), ]
dat.Recall = dat.Recall[order(dat.Recall$Stimuli.Cue), ]
dat.Recall = dat.Recall[order(dat.Recall$Condition.Number), ]
dat.Recall = dat.Recall[order(dat.Recall$Stimuli.Shuffle), ]
#Okay, put it back together now
dat.R = dat.Recall[ , c(12:14)]
#Drop overlapping columns and clean things up
JOL = cbind(dat.JOL, dat.R)
JOL = JOL[ , -c(11, 14, 17)]
JOL = JOL[ , -c(9:10)]
JOL = JOL[ , -2]
##Now do the same for the study only condition
#Start by subsetting out the recall and study trials for each dataset
dat2.Study = subset(dat2,
dat2$Procedure.Trial.Type == "Study")
dat2.Recall = subset(dat2,
dat2$Procedure.Trial.Type == "Test")
#get Study and Recall in the same order
dat2.Study = dat2.Study[order(dat2.Study$Stimuli.Cue), ]
dat2.Study = dat2.Study[order(dat2.Study$Condition.Number), ]
dat2.Study = dat2.Study[order(dat2.Study$Stimuli.Shuffle), ]
dat2.Recall = dat2.Recall[order(dat2.Recall$Stimuli.Cue), ]
dat2.Recall = dat2.Recall[order(dat2.Recall$Condition.Number), ]
dat2.Recall = dat2.Recall[order(dat2.Recall$Stimuli.Shuffle), ]
#Okay, put it back together now
dat2.R = dat2.Recall[ , c(12:14)]
#Drop overlapping columns and clean things up
Study = cbind(dat2.Study, dat2.R)
Study = Study[ , -c(11, 14, 16)]
Study = Study[ , -c(9:10)]
Study = Study[ , -2]
####Score the recall data####
##Going to write everything to .csv and then use the old shiny app to score
#first lowercase everything
JOL$Response.Response[JOL$Response.Response == 	"beyonc�"] = ""
JOL$Stimuli.Cue = tolower(JOL$Stimuli.Cue)
JOL$Stimuli.Answer = tolower(JOL$Stimuli.Answer)
Study$Stimuli.Cue = tolower(Study$Stimuli.Cue)
Study$Stimuli.Answer = tolower(Study$Stimuli.Answer)
JOL$Response.Response = tolower(JOL$Response.Response)
Study$Response.Response = tolower(Study$Response.Response)
#now write to .csv for scoring
length(unique(JOL$Username)) #22
length(unique(Study$Username)) #22
#add source
JOL$source = rep("USM")
Study$source = rep("USM")
#write.csv(JOL[ , c(1, 12, 5, 2:4, 6:11, 13)], file = "JOL_pre_scored_usm.csv", row.names = F)
#write.csv(Study[ , c(1, 11, 5, 2:4, 6:10, 12)], file = "Study_pre_scored_usm.csv", row.names = F)
35+18
write.csv(JOL[ , c(1, 12, 5, 2:4, 6:11, 13)], file = "JOL_pre_scored_usm.csv", row.names = F)
write.csv(Study[ , c(1, 11, 5, 2:4, 6:10, 12)], file = "Study_pre_scored_usm.csv", row.names = F)
